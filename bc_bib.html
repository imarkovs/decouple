<h1>bc.bib</h1><a name="hollander2017lvaica"></a><pre>
@incollection{<a href="bc.html#hollander2017lvaica">hollander2017lvaica</a>,
  author = {G. Hollander and P. Dreesen and M. Ishteva and J. Schoukens},
  title = {An initialization method for nonlinear model reduction},
  booktitle = {Latent Variable Analysis and Signal Separation},
  series = {Lecture Notes on Computer Science},
  volume = {10169},
  pages = {111--120},
  year = {2017}
}
</pre>

<a name="dreesen2017lvaica"></a><pre>
@incollection{<a href="bc.html#dreesen2017lvaica">dreesen2017lvaica</a>,
  author = {P. Dreesen and D. T. Westwick and J. Schoukens and M. Ishteva},
  title = {Modeling parallel {W}iener-{H}ammerstein systems using tensor decomposition of {V}olterra kernels},
  booktitle = {Latent Variable Analysis and Signal Separation},
  series = {Lecture Notes in Computer Science},
  volume = {10169},
  pages = {16--25},
  year = {2017}
}
</pre>

<a name="dreesen2015lvaica"></a><pre>
@incollection{<a href="bc.html#dreesen2015lvaica">dreesen2015lvaica</a>,
  author = {Dreesen, P. and Goossens, T. and Ishteva, M. and De Lathauwer, L. and Schoukens, J.},
  title = {Block-Decoupling Multivariate Polynomials Using the Tensor Block-Term Decomposition},
  year = {2015},
  isbn = {978-3-319-22481-7},
  booktitle = {Latent Variable Analysis and Signal Separation},
  volume = {9237},
  series = {Lecture Notes in Computer Science},
  editor = {Vincent, E. and Yeredor, A. and Koldovsk\'y, Z. and Tichavsk\'y, P.},
  doi = {10.1007/978-3-319-22482-4_2},
  url = {<a href="http://dx.doi.org/10.1007/978-3-319-22482-4_2">http://dx.doi.org/10.1007/978-3-319-22482-4_2</a>},
  publisher = {Springer},
  keywords = {Multivariate polynomials; Multilinear algebra; Tensor decomposition; Block-term decomposition; Waring decomposition},
  pages = {14--21},
  pdf = {<a href="http://homepages.vub.ac.be/~mishteva/papers/Block-decoupling%20_multivariate_polynomials_LVA-ICA_2015.pdf">http://homepages.vub.ac.be/~mishteva/papers/Block-decoupling%20_multivariate_polynomials_LVA-ICA_2015.pdf</a>}
}
</pre>

<a name="ica18a"></a><pre>
@incollection{<a href="bc.html#ica18a">ica18a</a>,
  author = {I. Markovsky and A. Fazzi and N. Guglielmi},
  title = {Applications of polynomial common factor computation in signal processing},
  booktitle = {Latent Variable Analysis and Signal Separation},
  publisher = {Springer},
  optvolume = {10891},
  series = {Lecture Notes in Computer Science},
  opteditor = {Y. Deville et al.},
  optisbn = {??},
  pages = {99--106},
  year = {2018},
  pdf = {<a href="http://homepages.vub.ac.be/~imarkovs/publications/ica18a.pdf">http://homepages.vub.ac.be/~imarkovs/publications/ica18a.pdf</a>},
  abstract = {We consider the problem of computing the greatest common divisor of a set of univariate polynomials and present applications of this problem in system theory and signal processing. One application is blind system identification: given the responses of a system to unknown inputs, find the system. Assuming that the unknown system is finite impulse response and at least two experiments are done with inputs that have finite support and their Z-transforms have no common factors, the impulse response of the system can be computed up to a scaling factor as the greatest common divisor of the Z-transforms of the outputs. Other applications of greatest common divisor problem in system theory and signal processing are finding the distance of a system to the set of uncontrollable systems and common dynamics estimation in a multi-channel sum-of-exponentials model.},
  keywords = {blind system identification; sum-of-exponentials modeling; distance to uncontrollability; approximate common factor; low-rank approximation},
  doi = {10.1007/978-3-319-93764-9_10}
}
</pre>

<a name="ica18b"></a><pre>
@incollection{<a href="bc.html#ica18b">ica18b</a>,
  author = {I. Markovsky and P.-L. Dragotti},
  title = {Using structured low-rank approximation for sparse signal recovery},
  booktitle = {Latent Variable Analysis and Signal Separation},
  publisher = {Springer},
  optvolume = {10891},
  series = {Lecture Notes in Computer Science},
  opteditor = {Y. Deville et al.},
  optisbn = {??},
  pages = {479--487},
  year = {2018},
  pdf = {<a href="http://homepages.vub.ac.be/~imarkovs/publications/ica18b.pdf">http://homepages.vub.ac.be/~imarkovs/publications/ica18b.pdf</a>},
  abstract = {Structured low-rank approximation is used in model reduction, system identification, and signal processing to find low-complexity models from data. The rank constraint imposes the condition that the approximation has bounded complexity and the optimization criterion aims to find the best match between the data---a trajectory of the system---and the approximation. In some applications, however, the data is sub-sampled from a trajectory, which poses the problem of sparse approximation using the the low-rank prior. This paper considers a modified structured low-rank approximation problem where the observed data is a linear transformation of a system's trajectory with reduced dimension. We reformulate this problem as a structured low-rank approximation with missing data and propose a solution methods based on the variable projections principle. We compare the structured low-rank approximation approach with the classical sparsity inducing method of $\ell_1$-norm regularization. The $\ell_1$-norm regularization method is effective for sum-of-exponentials modeling with a large number of samples, however, it is not suitable for identification of systems with damping.},
  keywords = {structured low-rank approximation, sparse approximation, missing data estimation, sum-of-exponentials modeling, $\ell_1$-norm regularization},
  software = {<a href="http://homepages.vub.ac.be/~imarkovs/software/ica18.tgz">http://homepages.vub.ac.be/~imarkovs/software/ica18.tgz</a>},
  doi = {10.1007/978-3-319-93764-9_44}
}
</pre>

<hr><p><em>This file was generated by
<a href="http://www.lri.fr/~filliatr/bibtex2html/">bibtex2html</a> 1.98.</em></p>
