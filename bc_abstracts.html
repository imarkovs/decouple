
<table>

<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="ica18a">1</a>]
</td>
<td class="bibtexitem">
I.&nbsp;Markovsky, A.&nbsp;Fazzi, and N.&nbsp;Guglielmi.
 Applications of polynomial common factor computation in signal
  processing.
 In <em>Latent Variable Analysis and Signal Separation</em>, Lecture
  Notes in Computer Science, pages 99--106. Springer, 2018.
[&nbsp;<a href="bc_bib.html#ica18a">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1007/978-3-319-93764-9_10">DOI</a>&nbsp;| 
<a href="http://homepages.vub.ac.be/~imarkovs/publications/ica18a.pdf">pdf</a>&nbsp;]
<blockquote>
We consider the problem of computing the greatest common divisor of a set of univariate polynomials and present applications of this problem in system theory and signal processing. One application is blind system identification: given the responses of a system to unknown inputs, find the system. Assuming that the unknown system is finite impulse response and at least two experiments are done with inputs that have finite support and their Z-transforms have no common factors, the impulse response of the system can be computed up to a scaling factor as the greatest common divisor of the Z-transforms of the outputs. Other applications of greatest common divisor problem in system theory and signal processing are finding the distance of a system to the set of uncontrollable systems and common dynamics estimation in a multi-channel sum-of-exponentials model.
</blockquote>
<p><blockquote>
Keywords: blind system identification; sum-of-exponentials modeling; distance to uncontrollability; approximate common factor; low-rank approximation
</blockquote>

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="ica18b">2</a>]
</td>
<td class="bibtexitem">
I.&nbsp;Markovsky and P.-L. Dragotti.
 Using structured low-rank approximation for sparse signal recovery.
 In <em>Latent Variable Analysis and Signal Separation</em>, Lecture
  Notes in Computer Science, pages 479--487. Springer, 2018.
[&nbsp;<a href="bc_bib.html#ica18b">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1007/978-3-319-93764-9_44">DOI</a>&nbsp;| 
<a href="http://homepages.vub.ac.be/~imarkovs/publications/ica18b.pdf">pdf</a>&nbsp;| 
<a href="http://homepages.vub.ac.be/~imarkovs/software/ica18.tgz">software</a>&nbsp;]
<blockquote>
Structured low-rank approximation is used in model reduction, system identification, and signal processing to find low-complexity models from data. The rank constraint imposes the condition that the approximation has bounded complexity and the optimization criterion aims to find the best match between the data---a trajectory of the system---and the approximation. In some applications, however, the data is sub-sampled from a trajectory, which poses the problem of sparse approximation using the the low-rank prior. This paper considers a modified structured low-rank approximation problem where the observed data is a linear transformation of a system's trajectory with reduced dimension. We reformulate this problem as a structured low-rank approximation with missing data and propose a solution methods based on the variable projections principle. We compare the structured low-rank approximation approach with the classical sparsity inducing method of <sub>1</sub>-norm regularization. The <sub>1</sub>-norm regularization method is effective for sum-of-exponentials modeling with a large number of samples, however, it is not suitable for identification of systems with damping.
</blockquote>
<p><blockquote>
Keywords: structured low-rank approximation, sparse approximation, missing data estimation, sum-of-exponentials modeling, <sub>1</sub>-norm regularization
</blockquote>

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="hollander2017lvaica">3</a>]
</td>
<td class="bibtexitem">
G.&nbsp;Hollander, P.&nbsp;Dreesen, M.&nbsp;Ishteva, and J.&nbsp;Schoukens.
 An initialization method for nonlinear model reduction.
 In <em>Latent Variable Analysis and Signal Separation</em>, volume 10169
  of <em>Lecture Notes on Computer Science</em>, pages 111--120. 2017.
[&nbsp;<a href="bc_bib.html#hollander2017lvaica">bib</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="dreesen2017lvaica">4</a>]
</td>
<td class="bibtexitem">
P.&nbsp;Dreesen, D.&nbsp;T. Westwick, J.&nbsp;Schoukens, and M.&nbsp;Ishteva.
 Modeling parallel Wiener-Hammerstein systems using tensor
  decomposition of Volterra kernels.
 In <em>Latent Variable Analysis and Signal Separation</em>, volume 10169
  of <em>Lecture Notes in Computer Science</em>, pages 16--25. 2017.
[&nbsp;<a href="bc_bib.html#dreesen2017lvaica">bib</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="dreesen2015lvaica">5</a>]
</td>
<td class="bibtexitem">
P.&nbsp;Dreesen, T.&nbsp;Goossens, M.&nbsp;Ishteva, L.&nbsp;De&nbsp;Lathauwer, and J.&nbsp;Schoukens.
 Block-decoupling multivariate polynomials using the tensor block-term
  decomposition.
 In E.&nbsp;Vincent, A.&nbsp;Yeredor, Z.&nbsp;Koldovsk&yacute;, and P.&nbsp;Tichavsk&yacute;,
  editors, <em>Latent Variable Analysis and Signal Separation</em>, volume 9237 of
  <em>Lecture Notes in Computer Science</em>, pages 14--21. Springer, 2015.
[&nbsp;<a href="bc_bib.html#dreesen2015lvaica">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1007/978-3-319-22482-4_2">DOI</a>&nbsp;| 
<a href="http://homepages.vub.ac.be/~mishteva/papers/Block-decoupling%20_multivariate_polynomials_LVA-ICA_2015.pdf">pdf</a>&nbsp;| 
<a href="http://dx.doi.org/10.1007/978-3-319-22482-4_2">http</a>&nbsp;]
<blockquote>
Keywords: Multivariate polynomials; Multilinear algebra; Tensor decomposition; Block-term decomposition; Waring decomposition
</blockquote>

</td>
</tr>
</table><hr><p><em>This file was generated by
<a href="http://www.lri.fr/~filliatr/bibtex2html/">bibtex2html</a> 1.98.</em></p>
